## Usage Examples

Below are practical examples of how to use the MongoDB provider with different search modes. These examples demonstrate the full workflow from initialization to querying.

### 1. Basic Vector Search

This example demonstrates semantic search using vector embeddings:

```python
import asyncio
from mongodb_llama_stack.mongodb.config import MongoDBIOConfig, SearchMode
from mongodb_llama_stack.mongodb.mongodb import MongoDBIOAdapter
from llama_stack.apis.vector_dbs import VectorDB
from llama_stack.apis.vector_io import Chunk

async def vector_search_example(inference_api):
    """Demonstrate basic vector search functionality"""
    
    # Initialize adapter with vector search configuration
    config = MongoDBIOConfig(
        connection_str="mongodb+srv://username:password@cluster.mongodb.net/",
        namespace="mydb.documents",
        search_mode=SearchMode.VECTOR,
        embeddings_key="embeddings",
        index_name="vector_index"
    )
    
    adapter = MongoDBIOAdapter(config, inference_api)
    await adapter.initialize()
    
    # Register a vector database
    vector_db = VectorDB(
        identifier="my_vector_db",
        embedding_dimension=1536,  # OpenAI embeddings dimension
        embedding_type="dense"
    )
    await adapter.register_vector_db(vector_db)
    
    # Insert documents with embeddings (embeddings will be generated by inference_api)
    chunks = [
        Chunk(
            content="The quick brown fox jumps over the lazy dog",
            metadata={"document_id": "doc1", "category": "animals"}
        ),
        Chunk(
            content="Machine learning is a subset of artificial intelligence",
            metadata={"document_id": "doc2", "category": "technology"}
        )
    ]
    
    await adapter.insert_chunks("my_vector_db", chunks)
    
    # Query for semantically similar documents
    query = "artificial intelligence and ML"
    results = await adapter.query_chunks(
        "my_vector_db",
        query,
        params={
            "max_chunks": 5,
            "score_threshold": 0.7,
            "filters": {"metadata.category": "technology"}  # Optional filtering
        }
    )
    
    # Process results
    for i, (chunk, score) in enumerate(zip(results.chunks, results.scores), 1):
        print(f"{i}. Score: {score:.3f} - {chunk.content[:100]}")
        print(f"   Category: {chunk.metadata.get('category')}")
    
    await adapter.shutdown()

# Run the example
if __name__ == "__main__":
    # You would need to provide a real inference API here
    from your_app import get_inference_api
    inference_api = get_inference_api()
    
    asyncio.run(vector_search_example(inference_api))
```
```

### 2. Hybrid Search with Custom Pipelines

This example shows how to leverage MongoDB's native rank fusion with multiple search pipelines:

```python
import asyncio
from mongodb_llama_stack.mongodb.config import MongoDBIOConfig, SearchMode, PipelineConfig
from mongodb_llama_stack.mongodb.mongodb import MongoDBIOAdapter
from llama_stack.apis.vector_dbs import VectorDB
from llama_stack.apis.vector_io import Chunk

async def hybrid_search_example(inference_api):
    """Demonstrate hybrid search with custom pipelines"""
    
    # Configure with custom pipelines for MongoDB 8.1+
    config = MongoDBIOConfig(
        connection_str="mongodb+srv://username:password@cluster.mongodb.net/",
        namespace="mydb.articles",
        search_mode=SearchMode.NATIVE_RANK_FUSION,
        rank_fusion_pipelines=[
            {
                "name": "semantic_search",
                "type": "vectorSearch",
                "weight": 2.0,  # Give more weight to semantic search
                "limit": 30,
                "config": {
                    "numCandidates": 200,
                    "index": "content_embeddings_index"
                }
            },
            {
                "name": "title_search",
                "type": "search",
                "weight": 1.5,
                "limit": 20,
                "config": {
                    "index": "title_text_index",
                    "operator": "phrase"
                }
            },
            {
                "name": "keyword_match",
                "type": "match",
                "weight": 1.0,
                "limit": 10,
                "config": {
                    "query": {"tags": {"$in": ["AI", "ML", "NLP"]}}
                }
            }
        ],
        enable_score_details=True  # Enable detailed scoring information
    )
    
    adapter = MongoDBIOAdapter(config, inference_api)
    await adapter.initialize()
    
    # Register vector database
    vector_db = VectorDB(
        identifier="article_db",
        embedding_dimension=1536,
        embedding_type="dense"
    )
    await adapter.register_vector_db(vector_db)
    
    # Insert sample articles
    articles = [
        {
            "title": "Advances in Transformer Architecture for NLP",
            "content": "Recent developments in transformer models have revolutionized natural language processing...",
            "tags": ["AI", "NLP", "Transformers"],
            "year": 2023
        },
        {
            "title": "Multimodal Learning with Vision-Language Models",
            "content": "Vision-language models combine computer vision and natural language processing capabilities...",
            "tags": ["AI", "ML", "Vision", "Multimodal"],
            "year": 2024
        },
        {
            "title": "Graph Neural Networks for Knowledge Representation",
            "content": "Graph neural networks provide a powerful framework for representing and reasoning about knowledge...",
            "tags": ["AI", "ML", "GNN", "Knowledge Graphs"],
            "year": 2023
        }
    ]
    
    # Convert to chunks for insertion
    chunks = []
    for article in articles:
        chunk = Chunk(
            content=article["content"],
            metadata={
                "title": article["title"],
                "tags": article["tags"],
                "year": article["year"]
            }
        )
        chunks.append(chunk)
    
    # Insert chunks
    await adapter.insert_chunks("article_db", chunks)
    
    # Query with both text and vector components
    query = "Latest developments in natural language processing"
    results = await adapter.query_chunks(
        "article_db",
        query,
        params={
            "max_chunks": 10,
            "mode": "hybrid",
            "filters": {"metadata.year": {"$gte": 2023}}
        }
    )
    
    # Process results with detailed scoring
    print(f"\nResults for query: '{query}'")
    print("-" * 60)
    
    for i, (chunk, score) in enumerate(zip(results.chunks, results.scores), 1):
        print(f"{i}. Combined Score: {score:.3f}")
        print(f"   Title: {chunk.metadata.get('title')}")
        print(f"   Tags: {', '.join(chunk.metadata.get('tags', []))}")
        print(f"   Year: {chunk.metadata.get('year')}")
        print(f"   Content: {chunk.content[:100]}...")
        
        # Access detailed scoring if available
        if hasattr(chunk, 'score_details'):
            print("   Score breakdown:")
            for pipeline, pipeline_score in chunk.score_details.items():
                print(f"     - {pipeline}: {pipeline_score:.3f}")
        
        print("-" * 60)
    
    await adapter.shutdown()
```

### 3. Graph-Enhanced Document Discovery

This example demonstrates how to use graph traversal to discover related documents:

```python
import asyncio
from mongodb_llama_stack.mongodb.config import MongoDBIOConfig, SearchMode, GraphLookupConfig
from mongodb_llama_stack.mongodb.mongodb import MongoDBIOAdapter
from llama_stack.apis.vector_dbs import VectorDB
from llama_stack.apis.vector_io import Chunk

async def graph_search_example(inference_api):
    """Demonstrate graph-enhanced document discovery"""
    
    # Configure with graph traversal capabilities
    config = MongoDBIOConfig(
        connection_str="mongodb+srv://username:password@cluster.mongodb.net/",
        namespace="mydb.knowledge_graph",
        search_mode=SearchMode.HYBRID_GRAPH,
        embeddings_key="embeddings",
        index_name="vector_index",
        
        # Configure graph traversal
        graph_lookup_config=GraphLookupConfig(
            from_collection=None,  # None = same collection
            start_with="$metadata.references",  # Starting point expression
            connect_from_field="metadata.references",  # Field to traverse from
            connect_to_field="metadata.doc_id",  # Field to match against
            as_field="related_docs",  # Output array field name
            max_depth=3,  # Maximum recursion depth
            depth_field="connection_depth",  # Store depth information
            restrict_search_with_match={  # Additional filters during traversal
                "metadata.quality_score": {"$gte": 0.8}
            }
        ),
        enable_graph_enhancement=True
    )
    
    adapter = MongoDBIOAdapter(config, inference_api)
    await adapter.initialize()
    
    # Register vector database
    vector_db = VectorDB(
        identifier="knowledge_graph_db",
        embedding_dimension=1536,
        embedding_type="dense"
    )
    await adapter.register_vector_db(vector_db)
    
    # Insert documents with relationships defined in metadata
    chunks = [
        Chunk(
            content="Introduction to Neural Networks: Neural networks are computational systems inspired by the human brain's neural structure. They consist of interconnected nodes or 'neurons' that process and transform input data to produce output predictions or classifications.",
            metadata={
                "doc_id": "nn_intro",
                "title": "Introduction to Neural Networks",
                "references": ["dl_basics", "backprop"],
                "quality_score": 0.95
            }
        ),
        Chunk(
            content="Deep Learning Fundamentals: Deep learning is a subset of machine learning that uses multi-layered neural networks to extract hierarchical features from data. These deep neural networks can automatically learn representations from data without explicit feature engineering.",
            metadata={
                "doc_id": "dl_basics",
                "title": "Deep Learning Fundamentals",
                "references": ["ml_intro", "nn_intro"],
                "quality_score": 0.90
            }
        ),
        Chunk(
            content="Backpropagation Algorithm Explained: Backpropagation is the primary algorithm for training neural networks. It works by calculating the gradient of the loss function with respect to each weight using the chain rule, then adjusting weights to minimize error.",
            metadata={
                "doc_id": "backprop",
                "title": "Backpropagation Algorithm",
                "references": ["calculus", "nn_intro"],
                "quality_score": 0.85
            }
        ),
        Chunk(
            content="Introduction to Machine Learning: Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from and make predictions on data without being explicitly programmed to perform specific tasks.",
            metadata={
                "doc_id": "ml_intro",
                "title": "Machine Learning Introduction",
                "references": ["ai_basics", "dl_basics"],
                "quality_score": 0.92
            }
        ),
        Chunk(
            content="Calculus for Machine Learning: Calculus is essential for understanding optimization algorithms in machine learning. Key concepts include derivatives, gradients, and chain rule which form the mathematical foundation of backpropagation.",
            metadata={
                "doc_id": "calculus",
                "title": "Calculus for ML",
                "references": ["math_basics"],
                "quality_score": 0.88
            }
        )
    ]
    
    # Insert chunks
    await adapter.insert_chunks("knowledge_graph_db", chunks)
    
    # Query using semantic search with graph traversal
    query = "neural network training algorithms"
    results = await adapter.query_chunks(
        "knowledge_graph_db",
        query,
        params={
            "max_chunks": 10,
            "enable_graph": True  # Enable graph traversal in query
        }
    )
    
    # Process results including graph connections
    print(f"\nResults for query: '{query}' (with graph traversal)")
    print("-" * 70)
    
    # Results include both direct matches and related documents found through graph traversal
    for i, chunk in enumerate(results.chunks, 1):
        # Get connection depth (0 means direct match, >0 means found through graph)
        depth = chunk.metadata.get('connection_depth', 0)
        depth_str = f"[Depth {depth}]" if depth > 0 else "[Direct match]"
        
        print(f"{i}. {depth_str} {chunk.metadata.get('title', 'Untitled')}")
        print(f"   Document ID: {chunk.metadata.get('doc_id', 'unknown')}")
        print(f"   Quality score: {chunk.metadata.get('quality_score', 'N/A')}")
        print(f"   Content: {chunk.content[:100]}...")
        
        # Show references
        refs = chunk.metadata.get('references', [])
        if refs:
            print(f"   References: {', '.join(refs)}")
        
        print("-" * 70)
    
    await adapter.shutdown()
```

### 4. Document Ingestion with Rich Metadata

This example demonstrates how to ingest documents with rich metadata and structured fields:

```python
import asyncio
import json
from datetime import datetime
from mongodb_llama_stack.mongodb.config import MongoDBIOConfig, SearchMode
from mongodb_llama_stack.mongodb.mongodb import MongoDBIOAdapter
from llama_stack.apis.vector_dbs import VectorDB
from llama_stack.apis.vector_io import Chunk

async def document_ingestion_example(inference_api):
    """Demonstrate document ingestion with rich metadata"""
    
    config = MongoDBIOConfig(
        connection_str="mongodb+srv://username:password@cluster.mongodb.net/",
        namespace="mydb.documents",
        search_mode=SearchMode.NATIVE_RANK_FUSION,
        embeddings_key="embeddings",
        index_name="content_vector_index",
        text_index_name="content_text_index",
        text_search_fields=["title", "content", "summary", "metadata.tags"]
    )
    
    adapter = MongoDBIOAdapter(config, inference_api)
    await adapter.initialize()
    
    # Register vector database
    vector_db = VectorDB(
        identifier="document_db",
        embedding_dimension=1536,
        embedding_type="dense"
    )
    await adapter.register_vector_db(vector_db)
    
    # Prepare documents with rich metadata structure
    documents = [
        {
            "title": "Introduction to Transformers",
            "content": "Transformers are a type of neural network architecture that uses self-attention mechanisms to process sequential data like text. Unlike recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), transformers process all tokens in parallel, making them more efficient for training on large datasets. The architecture consists of encoder and decoder stacks, with each layer containing self-attention and feed-forward neural networks.",
            "summary": "Overview of transformer models in NLP",
            "tags": ["AI", "NLP", "deep learning"],
            "metadata": {
                "document_id": "doc_001",
                "author": "John Doe",
                "date": "2024-01-15",
                "category": "research",
                "related_ids": ["doc_002", "doc_003"],
                "version": "1.0",
                "citations": 42,
                "language": "en-US",
                "source": "research_papers",
                "access_level": "public"
            }
        },
        {
            "title": "Fine-tuning Large Language Models",
            "content": "Fine-tuning large language models involves taking a pre-trained model and adapting it to specific tasks or domains. Techniques like parameter-efficient fine-tuning (PEFT), LoRA (Low-Rank Adaptation), and QLoRA (Quantized LoRA) allow efficient adaptation with limited computational resources. These approaches modify only a small subset of the model's parameters while keeping most of the pre-trained weights frozen.",
            "summary": "Techniques for adapting pre-trained models to specific tasks",
            "tags": ["LLM", "fine-tuning", "PEFT", "LoRA"],
            "metadata": {
                "document_id": "doc_002",
                "author": "Jane Smith",
                "date": "2024-02-20",
                "category": "tutorial",
                "related_ids": ["doc_001", "doc_004"],
                "version": "2.1",
                "citations": 18,
                "language": "en-US",
                "source": "tech_blog",
                "access_level": "premium"
            }
        },
        {
            "title": "Embedding Models for Retrieval",
            "content": "Embedding models convert text or other data into high-dimensional vector representations that capture semantic meaning. These vectors enable similarity search, where documents with similar meanings have vectors close to each other in the embedding space. Models like E5, BGE, and SentenceTransformers provide state-of-the-art embedding capabilities for retrieval augmented generation (RAG) systems.",
            "summary": "Vector embeddings for semantic search and retrieval",
            "tags": ["embeddings", "vector search", "RAG", "similarity"],
            "metadata": {
                "document_id": "doc_003",
                "author": "Alex Johnson",
                "date": "2024-03-05",
                "category": "technical",
                "related_ids": ["doc_001", "doc_005"],
                "version": "1.2",
                "citations": 27,
                "language": "en-US",
                "source": "documentation",
                "access_level": "public"
            }
        }
    ]
    
    # Convert to chunks with structured metadata
    chunks = []
    for doc in documents:
        # Prepare metadata by combining fields
        metadata = doc["metadata"].copy()
        
        # Add document fields to metadata
        metadata.update({
            "title": doc["title"],
            "summary": doc["summary"],
            "tags": doc["tags"],
            "ingestion_timestamp": datetime.now().isoformat()
        })
        
        # Create chunk
        chunk = Chunk(
            content=doc["content"],
            metadata=metadata
        )
        chunks.append(chunk)
    
    print(f"Preparing to ingest {len(chunks)} documents...")
    
    # Insert with automatic embedding generation
    await adapter.insert_chunks("document_db", chunks)
    
    print(f"âœ… Successfully ingested {len(chunks)} documents with metadata")
    
    # Demonstrate querying with metadata filters
    query = "transformer architecture for language models"
    results = await adapter.query_chunks(
        "document_db",
        query,
        params={
            "max_chunks": 5,
            "filters": {
                "metadata.access_level": "public",
                "metadata.citations": {"$gte": 20}
            }
        }
    )
    
    # Display query results
    print(f"\nQuery results for: '{query}' (with metadata filters)")
    print("-" * 70)
    
    for i, (chunk, score) in enumerate(zip(results.chunks, results.scores), 1):
        print(f"{i}. [{score:.3f}] {chunk.metadata.get('title')}")
        print(f"   Author: {chunk.metadata.get('author')}")
        print(f"   Category: {chunk.metadata.get('category')}")
        print(f"   Access level: {chunk.metadata.get('access_level')}")
        print(f"   Citations: {chunk.metadata.get('citations')}")
        print(f"   Tags: {', '.join(chunk.metadata.get('tags', []))}")
        print(f"   Summary: {chunk.metadata.get('summary')}")
        print(f"   Content: {chunk.content[:150]}...")
        print("-" * 70)
    
    await adapter.shutdown()

# Example runner
if __name__ == "__main__":
    # In a real application, you would use your actual inference API
    from your_app import get_inference_api
    inference_api = get_inference_api()
    
    asyncio.run(document_ingestion_example(inference_api))
```

## Index management

### Creating Indexes Manually

```javascript
## MongoDB Index Management

### Creating Indexes Manually

For optimal performance, you can manually create indexes in MongoDB:

```javascript
// Connect to MongoDB
use mydb

// Create vector search index for semantic search
db.mycollection.createSearchIndex(
  "vector_index",
  "vectorSearch",
  {
    "fields": [
      {
        "type": "vector",
        "path": "embeddings",
        "numDimensions": 1536,  // Match your embedding model's dimensions
        "similarity": "cosine"   // Or "dotProduct" or "euclidean"
      }
    ]
  }
)

// Create text search index for keyword search
db.mycollection.createSearchIndex(
  "text_index",
  "search",
  {
    "mappings": {
      "dynamic": true,
      "fields": {
        "title": {
          "type": "string",
          "analyzer": "lucene.standard"
        },
        "content": {
          "type": "string",
          "analyzer": "lucene.standard"
        },
        "metadata.description": {
          "type": "string", 
          "analyzer": "lucene.english"
        },
        "metadata.tags": {
          "type": "string",
          "analyzer": "lucene.keyword"
        }
      }
    }
  }
)

// Create compound index for efficient filtering
db.mycollection.createIndex(
  {
    "metadata.category": 1,
    "metadata.date": 1
  },
  {
    "name": "category_date_idx"
  }
)
```

### Advanced Index Configuration

For production workloads, consider these advanced index configurations:

```javascript
// Advanced vector search index with IVF configuration
db.mycollection.createSearchIndex(
  "advanced_vector_index",
  "vectorSearch",
  {
    "fields": [
      {
        "type": "vector",
        "path": "embeddings",
        "numDimensions": 1536,
        "similarity": "cosine",
        "vectorIndexConfig": {
          "type": "ivflat",
          "numLists": 100,       // Number of clusters (typically sqrt(n) where n is collection size)
          "similarity": "cosine"
        }
      }
    ]
  }
)

// Compound search index combining multiple fields
db.mycollection.createSearchIndex(
  "compound_text_index",
  "search",
  {
    "mappings": {
      "dynamic": false,
      "fields": {
        "content": {
          "type": "string",
          "analyzer": "lucene.english",
          "searchAnalyzer": "lucene.english",
          "multiValue": false,
          "store": false,
          "term_vector": "yes"
        },
        "title": {
          "type": "string",
          "analyzer": "lucene.english",
          "boost": 2.0  // Boost title matches
        },
        "metadata": {
          "type": "document",
          "fields": {
            "tags": {
              "type": "string",
              "analyzer": "lucene.keyword"
            },
            "category": {
              "type": "string", 
              "analyzer": "lucene.keyword"
            }
          }
        }
      }
    }
  }
)
```

### Checking Index Status

```javascript
// List all indexes
db.mycollection.getIndexes()

// List search indexes
db.mycollection.listSearchIndexes()

// Get detailed information about a specific search index
db.mycollection.aggregate([
  {
    $searchMeta: {
      index: "vector_index",
      count: { type: "total" }
    }
  }
])
```

// Create compound index for filtering
db.mycollection.createIndex({
  "metadata.category": 1,
  "metadata.date": -1
})
```